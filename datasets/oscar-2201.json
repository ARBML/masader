{
    "Name": "OSCAR-2201",
    "Subsets": [],
    "HF Link": "https://hf.co/datasets/oscar-corpus/OSCAR-2201",
    "Link": "https://hf.co/datasets/oscar-corpus/OSCAR-2201",
    "License": "CC0",
    "Year": 2022,
    "Language": "multilingual",
    "Dialect": "mixed",
    "Domain": [
        "web pages"
    ],
    "Form": "text",
    "Collection Style": [
        "crawling"
    ],
    "Description": "OSCAR or Open Super-large Crawled Aggregated coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the ungoliant architecture. Data is distributed by language in both original and deduplicated form.",
    "Volume": 8718929.0,
    "Unit": "documents",
    "Ethical Risks": "Low",
    "Provider": [],
    "Derived From": [
        "Common Crawl"
    ],
    "Paper Title": "Towards a Cleaner Document-Oriented Multilingual Crawled Corpus",
    "Paper Link": "https://arxiv.org/pdf/2201.06642.pdf",
    "Script": "Arab",
    "Tokenized": false,
    "Host": "HuggingFace",
    "Access": "Upon-Request",
    "Cost": "",
    "Test Split": false,
    "Tasks": [
        "text generation",
        "language modeling"
    ],
    "Venue Title": "arXiv",
    "Venue Type": "preprint",
    "Venue Name": "",
    "Authors": [
        "Julien Abadji",
        "Pedro Ortiz Suarez",
        "Laurent Romary",
        "Beno\u0131t Sagot"
    ],
    "Affiliations": [],
    "Abstract": "The need for raw large raw corpora has dramatically increased in recent years with the introduction of transfer learning and\nsemi-supervised learning methods to Natural Language Processing. And while there have been some recent attempts to\nmanually curate the amount of data necessary to train large language models, the main way to obtain this data is still through\nautomatic web crawling. In this paper we take the existing multilingual web corpus OSCAR and its pipeline Ungoliant that\nextracts and classifies data from Common Crawl at the line level, and propose a set of improvements and automatic annotations\nin order to produce a new document-oriented version of OSCAR that could prove more suitable to pre-train large generative\nlanguage models as well as hopefully other applications in Natural Language Processing and Digital Humanities.",
    "Added By": "Zaid Alyafeai"
}