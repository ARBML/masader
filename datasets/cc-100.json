{
    "Name": "CC-100",
    "Subsets": [],
    "HF Link": "https://hf.co/datasets/statmt/cc100",
    "Link": "https://data.statmt.org/cc-100/",
    "License": "unknown",
    "Year": 2020,
    "Language": "ar",
    "Dialect": "mixed",
    "Domain": "other",
    "Form": "text",
    "Collection Style": "crawling",
    "Description": "monolingual datasets from Common Crawl for a variety of languages",
    "Volume": "7,132,000",
    "Unit": "documents",
    "Ethical Risks": "Low",
    "Provider": "Facebook",
    "Derived From": "Common Crawl",
    "Paper Title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data",
    "Paper Link": "https://aclanthology.org/2020.lrec-1.494.pdf",
    "Script": "Arab",
    "Tokenized": "No",
    "Host": "other",
    "Access": "Free",
    "Cost": "",
    "Test Split": "No",
    "Tasks": "text generation, language modeling",
    "Venue Title": "LREC",
    "Citations": "",
    "Venue Type": "conference",
    "Venue Name": "Language Resources and Evaluation Conference",
    "Authors": "Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary,\nFrancisco Guzm\u00e1n, Armand Joulin, Edouard Grave",
    "Affiliations": "Facebook AI",
    "Abstract": "Pre-training text representations have led to significant improvements in many areas of natural language processing. The\nquality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this\npaper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for\na variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al.,\n2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select\ndocuments that are close to high quality corpora like Wikipedia",
    "Added By": "Zaid Alyafeai"
}