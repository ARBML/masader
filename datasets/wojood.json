{
    "Name": "Wojood",
    "Subsets": [],
    "HF Link": "",
    "Link": "https://ontology.birzeit.edu/Wojood/",
    "License": "custom",
    "Year": 2022,
    "Language": "ar",
    "Dialect": "Modern Standard Arabic",
    "Domain": "news articles",
    "Form": "text",
    "Collection Style": "crawling",
    "Description": "Wojood consists of about 550K tokens (MSA and dialect) that are manually annotated with 21 entity types (e.g., person, organization, location, event, date, etc). It covers multiple domains and was annotated with nested entities. The corpus contains about 75K entities and 22.5% of which are nested.",
    "Volume": "550,464",
    "Unit": "tokens",
    "Ethical Risks": "Low",
    "Provider": "SinaLab, Birzeit University",
    "Derived From": "",
    "Paper Title": "Wojood: Nested Arabic Named Entity Corpus and Recognition using BERT",
    "Paper Link": "https://arxiv.org/pdf/2205.09651.pdf",
    "Script": "Arab",
    "Tokenized": "No",
    "Host": "other",
    "Access": "Upon-Request",
    "Cost": "",
    "Test Split": "Yes",
    "Tasks": "named entity recognition",
    "Venue Title": "arXiv",
    "Citations": "",
    "Venue Type": "preprint",
    "Venue Name": "",
    "Authors": "Mustafa Jarrar, Mohammed Khalilia, Sana Ghanem",
    "Affiliations": "Birzeit University",
    "Abstract": "This paper presents Wojood, a corpus for Arabic nested Named Entity Recognition (NER). Nested entities occur when one\nentity mention is embedded inside another entity mention. Wojood consists of about 550K Modern Standard Arabic (MSA) and\ndialect tokens that are manually annotated with 21 entity types including person, organization, location, event and date. More\nimportantly, the corpus is annotated with nested entities instead of the more common flat annotations. The data contains about\n75K entities and 22.5% of which are nested. The inter-annotator evaluation of the corpus demonstrated a strong agreement\nwith Cohen\u2019s Kappa of 0.979 and an F1-score of 0.976. To validate our data, we used the corpus to train a nested NER model\nbased on multi-task learning using the pre-trained AraBERT (Arabic BERT). The model achieved an overall micro F1-score of\n0.884. Our corpus, the annotation guidelines, the source code and the pre-trained model are publicly available.\n",
    "Added By": "Zaid Alyafeai"
}