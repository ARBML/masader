{
    "Name": "EverGreenQA",
    "Subsets": [],
    "HF Link": "https://huggingface.co/datasets/s-nlp/EverGreen-Multilingual",
    "Link": "https://huggingface.co/datasets/s-nlp/EverGreen-Multilingual",
    "License": "unknown",
    "Year": 2025,
    "Language": "multilingual",
    "Dialect": "Modern Standard Arabic",
    "Domain": [
        "LLM"
    ],
    "Form": "text",
    "Collection Style": [
        "LLM generated"
    ],
    "Description": "Multilingual evergreen/mutable question\u2013answer dataset with 4.757 human-curated examples across 7 languages.",
    "Volume": 4757.0,
    "Unit": "sentences",
    "Ethical Risks": "Low",
    "Provider": [
        "Skoltech",
        "AIRI",
        "HSE University",
        "MTS AI",
        "MIPT"
    ],
    "Derived From": [],
    "Paper Title": "Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA",
    "Paper Link": "https://arxiv.org/pdf/2505.21115",
    "Script": "Arab",
    "Tokenized": false,
    "Host": "HuggingFace",
    "Access": "Free",
    "Cost": "",
    "Test Split": true,
    "Tasks": [
        "question answering"
    ],
    "Venue Title": "arXiv",
    "Venue Type": "preprint",
    "Venue Name": "arXiv",
    "Authors": [
        "Sergey Pletenev",
        "Maria Marina",
        "Nikolay Ivanov",
        "Daria Galimzianova",
        "Nikita Krayko",
        "Mikhail Salnikov",
        "Vasily Konovalov",
        "Alexander Panchenko",
        "Viktor Moskvoretskii"
    ],
    "Affiliations": [
        "Skoltech",
        "AIRI",
        "HSE University",
        "MTS AI",
        "MIPT"
    ],
    "Abstract": "Large Language Models (LLMs) often hallucinate in question answering (QA) tasks. A key yet under-explored factor contributing to this is the temporality of questions \u2013 whether they are evergreen (answers remain stable over time) or mutable (answers change). In this work, we introduce EverGreenQA, the first multilingual QA dataset with evergreen labels, supporting both evaluation and training. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they encode question temporality explicitly (via verbalized judgments) or implicitly (via uncertainty signals). We also train EG-E5, a lightweight multilingual classifier that achieves SoTA performance on this task. Finally, we demonstrate the practical utility of evergreen classification across three applications: improving self-knowledge estimation, filtering QA datasets, and explaining GPT-4o\u2019s retrieval behavior.",
    "Added By": "Zaid Alyafeai"
}