{
    "Name": "CVQA",
    "Subsets": [],
    "HF Link": "https://huggingface.co/datasets/afaji/cvqa",
    "Link": "https://huggingface.co/datasets/afaji/cvqa",
    "License": "custom",
    "Year": 2024,
    "Language": "multilingual",
    "Dialect": "Egypt",
    "Domain": [
        "other"
    ],
    "Form": "images",
    "Collection Style": [
        "human annotation",
        "manual curation"
    ],
    "Description": "A multilingual visual question answering benchmark covering 30 countries and 31 languages with 10k culturally-driven questions.",
    "Volume": 203.0,
    "Unit": "images",
    "Ethical Risks": "Low",
    "Provider": [
        "MBZUAI"
    ],
    "Derived From": [],
    "Paper Title": "CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark",
    "Paper Link": "https://arxiv.org/pdf/2406.05967",
    "Script": "Arab",
    "Tokenized": false,
    "Host": "HuggingFace",
    "Access": "Free",
    "Cost": "",
    "Test Split": false,
    "Tasks": [
        "multiple choice question answering"
    ],
    "Venue Title": "NeurIPS",
    "Venue Type": "conference",
    "Venue Name": "Conference on Neural Information Processing Systems",
    "Authors": [
        "David Romero",
        "Chenyang Lyu",
        "Haryo Akbarianto Wibowo",
        "Teresa Lynn",
        "Injy Hamed",
        "Aditya Nanda Kishore",
        "Aishik Mandal",
        "Alina Dragonetti",
        "Artem Abzaliev",
        "Atnafu Lambebo Tonja",
        "Bontu Fufa Balcha",
        "Chenxi Whitehouse",
        "Christian Salamea",
        "Dan John Velasco",
        "David Ifeoluwa Adelani",
        "David Le Meur",
        "Emilio Villa-Cueva",
        "Fajri Koto",
        "Fauzan Farooqui",
        "Frederico Belcavello",
        "Ganzorig Batnasan",
        "Gisela Vallejo",
        "Grainne Caulfield",
        "Guido Ivetta",
        "Haiyue Song",
        "Henok Biadglign Ademtew",
        "Hern\u00e1n Maina",
        "Holy Lovenia",
        "Israel Abebe Azime",
        "Jan Christian Blaise Cruz",
        "Jay Gala",
        "Jiahui Geng",
        "Jesus-German Ortiz-Barajas",
        "Jinheon Baek",
        "Jocelyn Dunstan",
        "Laura Alonso Alemany",
        "Kumaranage Ravindu Yasas Nagasinghe",
        "Luciana Benotti",
        "Luis Fernando D\u2019Haro",
        "Marcelo Viridiano",
        "Marcos Estecha-Garitagoitia",
        "Maria Camila Buitrago Cabrera",
        "Mario Rodr\u00edguez-Cantelar",
        "M\u00e9lanie Jouitteau",
        "Mihail Mihaylov",
        "Naome Etori",
        "Mohamed Fazli Mohamed Imam",
        "Muhammad Farid Adilazuarda",
        "Munkhjargal Gochoo",
        "Munkh-Erdene Otgonbold",
        "Olivier Niyomugisha",
        "Paula M\u00f3nica Silva",
        "Pranjal Chitale",
        "Raj Dabre",
        "Rendi Chevi",
        "Ruochen Zhang",
        "Ryandito Diandaru",
        "Samuel Cahyawijaya",
        "Santiago G\u00f3ngora",
        "Soyeong Jeong",
        "Sukannya Purkayastha",
        "Tatsuki Kuribayashi",
        "Teresa Clifford",
        "Thanmay Jayakumar",
        "Tiago Timponi Torrent",
        "Toqeer Ehsan",
        "Vladimir Araujo",
        "Yova Kementchedjhieva",
        "Zara Burzo",
        "Zheng Wei Lim",
        "Zheng Xin Yong",
        "Oana Ignat",
        "Joan Nwatu",
        "Rada Mihalcea",
        "Thamar Solorio",
        "Alham Fikri Aji"
    ],
    "Affiliations": [
        "MBZUAI",
        "IIT Madras",
        "TU Darmstadt",
        "Universidad de la Rep\u00fablica",
        "University of Michigan",
        "Independent Researcher",
        "University of Cambridge",
        "Samsung Research Philippines",
        "Bretagne num\u00e9rique",
        "Federal University of Juiz de Fora",
        "United Arab Emirates University",
        "University of Melbourne",
        "Dublin City University",
        "Universidad Nacional de C\u00f3rdoba",
        "NICT",
        "EAII",
        "AISingapore",
        "Saarland University",
        "KAIST",
        "Pontificia Universidad Cat\u00f3lica de Chile",
        "Universidad Polit\u00e9cnica de Madrid",
        "IKER, CNRS",
        "Millennium Institute Foundational Research on Data",
        "Brown University",
        "ITB",
        "HKUST",
        "TUDarmstadt",
        "KULeuven",
        "Skyline High School",
        "CNPq",
        "University of Michigan"
    ],
    "Abstract": "Visual Question Answering (VQA) is an important task in multimodal AI, and it is often used to test the ability of vision-language models to understand and reason on knowledge present in both visual and textual data. However, most of the current VQA models use datasets that are primarily focused on English and a few major world languages, with images that are typically Western-centric. While recent efforts have tried to increase the number of languages covered on VQA datasets, they still lack diversity in low-resource languages. More importantly, although these datasets often extend their linguistic range via translation or some other approaches, they usually keep images the same, resulting in narrow cultural representation. To address these limitations, we construct CVQA, a new Culturally-diverse multilingual Visual Question Answering benchmark, designed to cover a rich set of languages and cultures, where we engage native speakers and cultural experts in the data collection process. As a result, CVQA includes culturally-driven images and questions from across 30 countries on four continents, covering 31 languages with 13 scripts, providing a total of 10k questions. We then benchmark several Multimodal Large Language Models (MLLMs) on CVQA, and show that the dataset is challenging for the current state-of-the-art models. This benchmark can serve as a probing evaluation suite for assessing the cultural capability and bias of multimodal models and hopefully encourage more research efforts toward increasing cultural awareness and linguistic diversity in this field.\n",
    "Added By": "Zaid Alyafeai"
}
