{
    "Name": "Cross-lingual NER",
    "Subsets": [],
    "HF Link": "https://hf.co/datasets/arbml/Zero_Shot_Cross_Lingual_NER_ar",
    "Link": "https://github.com/ntunlp/Zero-Shot-Cross-Lingual-NER",
    "License": "unknown",
    "Year": 2020,
    "Language": "multilingual",
    "Dialect": "Modern Standard Arabic",
    "Domain": [
        "other"
    ],
    "Form": "text",
    "Collection Style": [
        "other"
    ],
    "Description": "NER for five different target languages\r\n\u2014 Spanish, Dutch, German, Arabic and Finnish",
    "Volume": 2687.0,
    "Unit": "tokens",
    "Ethical Risks": "Low",
    "Provider": [
        "Multiple Institutions"
    ],
    "Derived From": [
        "AQMAR"
    ],
    "Paper Title": "Zero-Resource Cross-Lingual Named Entity Recognition",
    "Paper Link": "https://arxiv.org/pdf/1911.09812.pdf",
    "Script": "Arab",
    "Tokenized": false,
    "Host": "GitHub",
    "Access": "Free",
    "Cost": "",
    "Test Split": true,
    "Tasks": [
        "named entity recognition"
    ],
    "Venue Title": "AAAI",
    "Venue Type": "conference",
    "Venue Name": "Association for the Advancement of Artificial Intelligence",
    "Authors": [
        "M SAIFUL BARI",
        "Shafiq R. Joty",
        "Prathyusha Jwalapuram"
    ],
    "Affiliations": [
        "Nanyang Technological University",
        "",
        ""
    ],
    "Abstract": "Recently, neural methods have achieved state-of-the-art (SOTA) results in Named Entity Recognition (NER) tasks for many languages without the need for manually crafted features. However, these models still require manually annotated training data, which is not available for many languages. In this paper, we propose an unsupervised cross-lingual NER model that can transfer NER knowledge from one language to another in a completely unsupervised way without relying on any bilingual dictionary or parallel data. Our model achieves this through word-level adversarial learning and augmented fine-tuning with parameter sharing and feature augmentation. Experiments on five different languages demonstrate the effectiveness of our approach, outperforming existing models by a good margin and setting a new SOTA for each language pair.",
    "Added By": "Zaid Alyafeai"
}