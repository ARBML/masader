{
    "Name": "MultiWikiQA",
    "Subsets": [],
    "HF Link": "https://huggingface.co/datasets/alexandrainst/multi-wiki-qa",
    "Link": "https://huggingface.co/datasets/alexandrainst/multi-wiki-qa",
    "License": "CC BY-NC-SA 4.0",
    "Year": 2025,
    "Language": "multilingual",
    "Dialect": "Modern Standard Arabic",
    "Domain": [
        "wikipedia"
    ],
    "Form": "text",
    "Collection Style": [
        "crawling",
        "machine annotation"
    ],
    "Description": "Multilingual reading-comprehension dataset with questions generated by an LLM from Wikipedia articles.",
    "Volume": 5000.0,
    "Unit": "sentences",
    "Ethical Risks": "Low",
    "Provider": [
        "Alexandra Institute"
    ],
    "Derived From": [],
    "Paper Title": "MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages",
    "Paper Link": "https://arxiv.org/pdf/2509.04111",
    "Script": "Arab",
    "Tokenized": false,
    "Host": "HuggingFace",
    "Access": "Free",
    "Cost": "",
    "Test Split": false,
    "Tasks": [
        "question answering"
    ],
    "Venue Title": "arXiv",
    "Venue Type": "preprint",
    "Venue Name": "arXiv",
    "Authors": [
        "Dan Saattrup Nielsen"
    ],
    "Affiliations": [
        "Alexandra Institute"
    ],
    "Abstract": "We introduce a new reading comprehension dataset, dubbed MultiWikiQA, which covers 306 languages. The context data comes from Wikipedia articles, with questions generated by an LLM and the answers appearing verbatim in the Wikipedia articles. We conduct a crowdsourced human evaluation of the fluency of the generated questions across 30 of the languages, providing evidence that the questions are of good quality. We evaluate six different language models\u2014both decoder and encoder models of varying sizes\u2014showing that the benchmark is sufficiently difficult and that there is a large performance discrepancy amongst the languages. The dataset and survey evaluations are freely available.",
    "Added By": "Zaid Alyafeai"
}