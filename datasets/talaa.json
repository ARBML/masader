{
    "Name": "TALAA",
    "Subsets": [],
    "HF Link": "https://hf.co/datasets/arbml/TALAA",
    "Link": "https://github.com/saidziani/Arabic-News-Article-Classification",
    "License": "unknown",
    "Year": 2015,
    "Language": "ar",
    "Dialect": "ar-MSA: (Arabic (Modern Standard Arabic))",
    "Domain": "news articles",
    "Form": "text",
    "Collection Style": "crawling",
    "Description": "collections of articles ",
    "Volume": "57,827",
    "Unit": "documents",
    "Ethical Risks": "Low",
    "Provider": "USTHB Algeria",
    "Derived From": "",
    "Paper Title": "Building TALAA, a Free General and Categorized Arabic Corpus \r",
    "Paper Link": "https://www.scitepress.org/Papers/2015/53521/53521.pdf",
    "Script": "Arab",
    "Tokenized": "No",
    "Host": "GitHub",
    "Access": "Free",
    "Cost": "",
    "Test Split": "Yes",
    "Tasks": "topic classification",
    "Venue Title": "ICAART",
    "Citations": "4.0",
    "Venue Type": "conference",
    "Venue Name": "Conference on Agents and Artificial Intelligence",
    "Authors": "Essma Selab,A. Guessoum",
    "Affiliations": ",",
    "Abstract": "Arabic natural language processing (ANLP) has gained increasing interest over the last decade. However, \n  \n the development of ANLP tools depends on the availability of large corpora. It turns out unfortunately that \n  \n the scientific community has a deficit in large and varied Arabic corpora, especially ones that are freely \n  \n accessible. With the Internet continuing its exponential growth, Arabic Internet content has also been \n  \n following the trend, yielding large amounts of textual data available through different Arabic websites. This \n  \n paper describes the TALAA corpus, a voluminous general Arabic corpus, built from daily Arabic \n  \n newspaper websites. The corpus is a collection of more than 14 million words with 15,891,729 tokens \n  \n contained in 57,827 different articles. A part of the TALAA corpus has been tagged to construct an \n  \n annotated Arabic corpus of about 7000 tokens, the POS-tagger used containing a set of 58 detailed tags. The \n  \n annotated corpus was manually checked by two human experts. The methodology used to construct TALAA \n  \n is presented and various metrics are applied to it, showing the usefulness of the corpus. The corpus can be \n  \n made available to the scientific community upon authorisation.",
    "Added By": ""
}