{
    "Name": "QAmeleon",
    "Subsets": [],
    "HF Link": "https://huggingface.co/datasets/imvladikon/QAmeleon",
    "Link": "https://github.com/google-research-datasets/QAmeleon",
    "License": "CC BY 4.0",
    "Year": 2023,
    "Language": "ar",
    "Dialect": "Modern Standard Arabic",
    "Domain": [
        "wikipedia"
    ],
    "Form": "text",
    "Collection Style": [
        "LLM generated"
    ],
    "Description": "Synthetic Arabic QA pairs generated by prompting PaLM-540B with 5 gold examples",
    "Volume": 6970.0,
    "Unit": "sentences",
    "Ethical Risks": "Low",
    "Provider": [
        "Google Research"
    ],
    "Derived From": [],
    "Paper Title": "QAmeleon: Multilingual QA with Only 5 Examples",
    "Paper Link": "https://arxiv.org/pdf/2211.08264",
    "Script": "Arab",
    "Tokenized": false,
    "Host": "GitHub",
    "Access": "Free",
    "Cost": "",
    "Test Split": false,
    "Tasks": [
        "question answering"
    ],
    "Venue Title": "arXiv",
    "Venue Type": "preprint",
    "Venue Name": "arXiv",
    "Authors": [
        "Priyanka Agrawal",
        "Chris Alberti",
        "Fantine Huot",
        "Joshua Maynez",
        "Ji Ma",
        "Sebastian Ruder",
        "Kuzman Ganchev",
        "Dipanjan Das",
        "Mirella Lapata"
    ],
    "Affiliations": [
        "Google Research"
    ],
    "Abstract": "The availability of large, high-quality datasets has been one of the main drivers of recent progress in question answering (QA). Such annotated datasets however are difficult and costly to collect, and rarely exist in languages other than English, rendering QA technology inaccessible to underrepresented languages. An alternative to building large monolingual training datasets is to leverage pre-trained language models (PLMs) under a few-shot learning setting. Our approach, QAmeleon, uses a PLM to automatically generate multilingual data upon which QA models are trained, thus avoiding costly annotation. Prompt tuning the PLM for data synthesis with only five examples per language delivers accuracy superior to translation-based baselines, bridges nearly 60% of the gap between an English-only baseline and a fully supervised upper bound trained on almost 50,000 hand labeled examples, and always leads to substantial improvements compared to fine-tuning a QA model directly on labeled examples in low resource settings. Experiments on the TyDiQA-GoldP and MLQA benchmarks show that few-shot prompt tuning for data synthesis scales across languages and is a viable alternative to large-scale annotation.",
    "Added By": "Zaid Alyafeai"
}