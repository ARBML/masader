{
    "Name": "101BAW",
    "Subsets": [],
    "HF Link": "https://huggingface.co/datasets/ClusterlabAi/101_billion_arabic_words_dataset",
    "Link": "https://huggingface.co/datasets/ClusterlabAi/101_billion_arabic_words_dataset",
    "License": "Apache-2.0",
    "Year": 2024,
    "Language": "ar",
    "Dialect": "Modern Standard Arabic",
    "Domain": [
        "web pages"
    ],
    "Form": "text",
    "Collection Style": [
        "crawling",
        "manual curation"
    ],
    "Description": "Web-scraped Arabic corpus of 101 billion tokens scraped from Common Crawl WET files and cleaned/deduplicated for training authentic Arabic LLMs.",
    "Volume": 101000000000.0,
    "Unit": "tokens",
    "Ethical Risks": "Medium",
    "Provider": [
        "Clusterlab"
    ],
    "Derived From": [
        "Common Crawl"
    ],
    "Paper Title": "101 Billion Arabic Words Dataset",
    "Paper Link": "https://arxiv.org/pdf/2504.21677",
    "Script": "Arab",
    "Tokenized": false,
    "Host": "HuggingFace",
    "Access": "Free",
    "Cost": "",
    "Test Split": false,
    "Tasks": [
        "language modeling"
    ],
    "Venue Title": "arXiv",
    "Venue Type": "preprint",
    "Venue Name": "",
    "Authors": [
        "Manel Aloui",
        "Hasna Chouikhi",
        "Ghaith Chaabane",
        "Haithem Kchaou",
        "Chehir Dhaouadi"
    ],
    "Affiliations": [
        "Clusterlab.ai"
    ],
    "Abstract": "In recent years, Large Language Models (LLMs) have revolutionized the field of natural language processing, showcasing an impressive rise predominantly in English-centric domains. These advancements have set a global benchmark, inspiring significant efforts toward developing Arabic LLMs capable of understanding and generating the Arabic language with remarkable accuracy. Despite these advancements, a critical challenge persists: the potential bias in Arabic LLMs, primarily attributed to their reliance on datasets comprising English data that has been translated into Arabic. This reliance not only compromises the authenticity of the generated content but also reflects a broader issue\u2014the scarcity of original quality Arabic linguistic data. This study aims to address the data scarcity in the Arab world and to encourage the development of Arabic Language Models that are true to both the linguistic and nuances of the region. We undertook a large-scale data mining project, extracting a substantial volume of text from the Common Crawl WET files, specifically targeting Arabic content. The extracted data underwent a rigorous cleaning and deduplication process, using innovative techniques to ensure the integrity and uniqueness of the dataset. The result is the 101 Billion Arabic Words Dataset, the largest Arabic dataset available to date, which can significantly contribute to the development of authentic Arabic LLMs. This study not only highlights the potential for creating linguistically and culturally accurate Arabic LLMs but also sets a precedent for future research in enhancing the authenticity of Arabic language models."
}