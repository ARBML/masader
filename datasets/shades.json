{
    "Name": "SHADES",
    "Subsets": [],
    "HF Link": "https://huggingface.co/datasets/LanguageShades/BiasShades",
    "Link": "https://huggingface.co/datasets/LanguageShades/BiasShades",
    "License": "custom",
    "Year": 2025,
    "Language": "multilingual",
    "Dialect": "Modern Standard Arabic",
    "Domain": [
        "other"
    ],
    "Form": "text",
    "Collection Style": [
        "human annotation",
        "manual curation"
    ],
    "Description": "Multilingual dataset of stereotypes across 16 languages and 37 regions to evaluate bias in LLMs.",
    "Volume": 304.0,
    "Unit": "sentences",
    "Ethical Risks": "High",
    "Provider": [],
    "Derived From": [],
    "Paper Title": "SHADES: Towards a Multilingual Assessment of Stereotypes in Large Language Models",
    "Paper Link": "https://arxiv.org/pdf/2310.16621",
    "Script": "Arab",
    "Tokenized": false,
    "Host": "HuggingFace",
    "Access": "Upon-Request",
    "Cost": "",
    "Test Split": false,
    "Tasks": [
        "safety evaluation"
    ],
    "Venue Title": "NAACL-HLT",
    "Venue Type": "conference",
    "Venue Name": "North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    "Authors": [
        "Margaret Mitchell",
        "Hamdan Al-Ali",
        "Giuseppe Attanasio",
        "Ioana Baldini",
        "Miruna Clinciu",
        "Jordan Clive",
        "Pieter Delobelle",
        "Manan Dey",
        "Kaustubh Dhole",
        "Timm Dill",
        "Amirbek Djanibekov",
        "Tair Djanibekov",
        "Jad Doughman",
        "Ritam Dutt",
        "Jessica Zosa Forde",
        "Jay Gala",
        "Avijit Ghosh",
        "Sil Hamilton",
        "Carolin Holtermann",
        "Jerry Huang",
        "Lucie-Aim\u00e9e Kaffee",
        "Janavi Kasera",
        "Tanmay Laud",
        "Anne Lauscher",
        "Roberto Luis L\u00f3pez",
        "Jonibek Mansurov",
        "Maraim Masoud",
        "Sagnik Mukherjee",
        "Nurdaulet Mukhituly",
        "Nikita Nangia",
        "Shangrui Nie",
        "Anaelia Ovalle",
        "Giada Pistilli",
        "Esther Ploeger",
        "Jeremy Qin",
        "Dragomir Radev",
        "Vipul Raheja",
        "Beatrice Savoldi",
        "Shanya Sharma",
        "Xudong Shen",
        "Karolina Sta\u0144czak",
        "Arjun Subramonian",
        "Kaiser Sun",
        "Eliza Szczechla",
        "Tiago Timponi Torrent",
        "Deepak Tunuguntla",
        "Emilio Villa-Cueva",
        "Marcelo Viridiano",
        "Oskar van der Wal",
        "Adina Yakefu",
        "Kayo Yin",
        "Mike Zhang",
        "Sydney Zink",
        "Aur\u00e9lie N\u00e9v\u00e9ol",
        "Zeerak Talat"
    ],
    "Affiliations": [
        "HuggingFace",
        "Mohamed bin Zayed University of Artificial Intelligence",
        "Instituto de Telecomunica\u00e7\u00f5es",
        "IBM Research",
        "Heriot-Watt University",
        "University of Edinburgh",
        "Imperial College London",
        "KU Leuven",
        "Salesforce",
        "Emory University",
        "Universit\u00e4t Hamburg",
        "KAIST AI",
        "Carnegie Mellon University",
        "Brown University",
        "Cornell University",
        "MILA",
        "Universit\u00e9 de Montr\u00e9al",
        "Boston University",
        "Hippocratic AI",
        "University of California, San Diego",
        "University of Bonn",
        "Google",
        "National University of Singapore",
        "Johns Hopkins University",
        "Universidade Federal de Juiz de Fora",
        "Yale University",
        "Grammarly",
        "Fondazione Bruno Kessler",
        "University of California, Los Angeles",
        "Aalborg University",
        "McGill",
        "Office of Court Administration of Puerto Rico",
        "University of Illinois Urbana-Champaign",
        "Amazon"
    ],
    "Abstract": "Large Language Models (LLMs) reproduce and exacerbate the social biases present in their training data, and resources to quantify this issue are limited. While research has attempted to identify and mitigate such biases, most efforts have been concentrated around English, lagging the rapid advancement of LLMs in multilingual settings. In this paper, we introduce a new multilingual parallel dataset SHADES to help address this issue, designed for examining culturally-specific stereotypes that may be learned by LLMs. The dataset includes stereotypes from 20 regions around the world and 16 languages, spanning multiple identity categories subject to discrimination worldwide. We demonstrate its utility in a series of exploratory evaluations for both \u201cbase\u201d and \u201cinstruction-tuned\u201d language models. Our results suggest that stereotypes are consistently reflected across models and languages, with some languages and models indicating much stronger stereotype biases than others.",
    "Added By": "Zaid Alyafeai"
}