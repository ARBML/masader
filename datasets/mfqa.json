{
    "Name": "MFQA",
    "Subsets": [],
    "HF Link": "https://hf.co/datasets/clips/mqa",
    "Link": "https://hf.co/datasets/clips/mqa",
    "License": "CC0",
    "Year": 2021,
    "Language": "multilingual",
    "Dialect": "mixed",
    "Domain": [
        "web pages"
    ],
    "Form": "text",
    "Collection Style": [
        "crawling"
    ],
    "Description": "MQA is a Multilingual corpus of Questions and Answers (MQA) parsed from the Common Crawl. Questions are divided in two types: Frequently Asked Questions (FAQ) and Community Question Answering (CQA).",
    "Volume": 3017456.0,
    "Unit": "sentences",
    "Ethical Risks": "Low",
    "Provider": [
        "University of Antwerp"
    ],
    "Derived From": [
        "Common Crawl"
    ],
    "Paper Title": "MFAQ: a Multilingual FAQ Dataset",
    "Paper Link": "https://arxiv.org/pdf/2109.12870.pdf",
    "Script": "Arab",
    "Tokenized": false,
    "Host": "HuggingFace",
    "Access": "Free",
    "Cost": "",
    "Test Split": false,
    "Tasks": [
        "question answering"
    ],
    "Venue Title": "arXiv",
    "Venue Type": "preprint",
    "Venue Name": "",
    "Authors": [
        "Maxime De Bruyn",
        "Ehsan Lotfi",
        "Jeska Buhmann",
        "Walter Daelemans"
    ],
    "Affiliations": [
        "CLiPS Research Center University of Antwerp"
    ],
    "Abstract": "In this paper, we present the first multilingual\nFAQ dataset publicly available. We collected\naround 6M FAQ pairs from the web, in 21 different languages. Although this is significantly\nlarger than existing FAQ retrieval datasets, it\ncomes with its own challenges: duplication of\ncontent and uneven distribution of topics. We\nadopt a similar setup as Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) and test\nvarious bi-encoders on this dataset. Our experiments reveal that a multilingual model based\non XLM-RoBERTa (Conneau et al., 2019)\nachieves the best results, except for English.\nLower resources languages seem to learn from\none another as a multilingual model achieves a\nhigher MRR than language-specific ones. Our\nqualitative analysis reveals the brittleness of\nthe model on simple word changes. We publicly release our dataset1\n, model2\nand training\nscript",
    "Added By": "Zaid Alyafeai"
}