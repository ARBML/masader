{
    "Name": "SWIM-IR",
    "Subsets": [],
    "HF Link": "https://huggingface.co/datasets/nthakur/swim-ir-cross-lingual",
    "Link": "https://github.com/google-research-datasets/SWIM-IR",
    "License": "CC BY-SA 4.0",
    "Year": 2024,
    "Language": "multilingual",
    "Dialect": "Modern Standard Arabic",
    "Domain": [
        "wikipedia"
    ],
    "Form": "text",
    "Collection Style": [
        "machine annotation"
    ],
    "Description": "Synthetic multilingual retrieval training dataset with 28M query-passage pairs across 33 languages generated by PaLM 2 without human supervision.",
    "Volume": 901000.0,
    "Unit": "documents",
    "Ethical Risks": "Low",
    "Provider": [
        "Google Research"
    ],
    "Derived From": [],
    "Paper Title": "Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval",
    "Paper Link": "https://arxiv.org/pdf/2311.05800",
    "Script": "Arab",
    "Tokenized": false,
    "Host": "GitHub",
    "Access": "Free",
    "Cost": "",
    "Test Split": false,
    "Tasks": [
        "cross-lingual information retrieval",
        "information retrieval"
    ],
    "Venue Title": "arXiv",
    "Venue Type": "preprint",
    "Venue Name": "arXiv",
    "Authors": [
        "Nandan Thakur",
        "Jianmo Ni",
        "Gustavo Hern\u00e1ndez \u00c1brego",
        "John Wieting",
        "Jimmy Lin",
        "Daniel Cer"
    ],
    "Affiliations": [
        "Google Research",
        "Google DeepMind",
        "University of Waterloo"
    ],
    "Abstract": "There has been limited success for dense re-\ntrieval models in multilingual retrieval, due\nto uneven and scarce training data available\nacross multiple languages. Synthetic training\ndata generation is promising (e.g., InPars or\nPromptagator), but has been investigated only\nfor English. Therefore, to study model capa-\nbilities across both cross-lingual and mono-\nlingual retrieval tasks, we develop SWIM-\nIR, a synthetic retrieval training dataset con-\ntaining 33 (high to very-low resource) lan-\nguages for fine-tuning multilingual dense re-\ntrievers without requiring any human super-\nvision. To construct SWIM-IR, we propose\nSAP (summarize-then-ask prompting), where\nthe large language model (LLM) generates a\ntextual summary prior to the query genera-\ntion step. SAP assists the LLM in generat-\ning informative queries in the target language.\nUsing SWIM-IR, we explore synthetic fine-\ntuning of multilingual dense retrieval models\nand evaluate them robustly on three retrieval\nbenchmarks: XOR-Retrieve (cross-lingual),\nMIRACL (monolingual) and XTREME-UP\n(cross-lingual). Our models, called SWIM-\nX, are competitive with human-supervised\ndense retrieval models, e.g., mContriever-X,\nfinding that SWIM-IR can cheaply substitute\nfor expensive human-labeled retrieval training\ndata. SWIM-IR dataset and SWIM-X mod-\nels are available at: https://github.com/\ngoogle-research-datasets/SWIM-IR",
    "Added By": "Zaid Alyafeai"
}
