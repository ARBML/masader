{
    "Name": "FineWeb2-HQ",
    "Subsets": [],
    "HF Link": "https://huggingface.co/datasets/epfml/FineWeb2-HQ",
    "Link": "https://huggingface.co/datasets/epfml/FineWeb2-HQ",
    "License": "ODC-By",
    "Year": 2025,
    "Language": "multilingual",
    "Dialect": "mixed",
    "Domain": [
        "public datasets"
    ],
    "Form": "text",
    "Collection Style": [
        "machine annotation"
    ],
    "Description": "A filtered subset of CommonCrawl focusing on 20 languages including Arabic with 127B tokens.",
    "Volume": 380138261.0,
    "Unit": "documents",
    "Ethical Risks": "Medium",
    "Provider": [
        "EPFL"
    ],
    "Derived From": [
        "FineWeb-2"
    ],
    "Paper Title": "Enhancing Multilingual LLM Pretraining with Model-Based Data Selection",
    "Paper Link": "https://arxiv.org/pdf/2502.10361",
    "Script": "Arab",
    "Tokenized": false,
    "Host": "HuggingFace",
    "Access": "Free",
    "Cost": "",
    "Test Split": false,
    "Tasks": [
        "language modeling"
    ],
    "Venue Title": "arXiv",
    "Venue Type": "preprint",
    "Venue Name": "arXiv",
    "Authors": [
        "Bettina Messmer",
        "Vinko Sabol\u010dec",
        "Martin Jaggi"
    ],
    "Affiliations": [
        "EPFL"
    ],
    "Abstract": "Dataset curation has become a basis for strong large language model (LLM) performance. While various rule-based filtering heuristics exist for English and multilingual datasets, model-based filtering techniques have primarily focused on English. To address the disparity stemming from limited research on non-English languages, we propose a model-based filtering framework for multilingual datasets that aims to identify a diverse set of structured and knowledge-rich samples. Our approach emphasizes transparency, simplicity, and efficiency, leveraging Transformer- and FastText-based classifiers to ensure the broad accessibility of our technique and data. We conduct comprehensive ablation studies on the FineWeb-2 web crawl dataset across diverse language families, scripts, and resource availability to demonstrate the effectiveness of our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our approach can match the baseline MMLU score with as little as 15% of the training tokens, while also improving across other benchmarks. These findings provide strong evidence for the generalizability of our approach to other languages. As a result, we extend our framework to 20 languages for which we release the refined pretraining datasets.\n",
    "Added By": "Zaid Alyafeai"
}