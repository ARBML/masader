{
    "Name": "wikimedia/wit_base",
    "Subsets": [],
    "HF Link": "https://hf.co/datasets/wikimedia/wit_base",
    "Link": "https://github.com/google-research-datasets/wit",
    "License": "CC BY-SA 4.0",
    "Year": 2021,
    "Language": "multilingual",
    "Dialect": "ar-MSA: (Arabic (Modern Standard Arabic))",
    "Domain": "wikipedia",
    "Form": "text",
    "Collection Style": "crawling",
    "Description": "Wikimedia's version of the Wikipedia-based Image Text (WIT) Dataset, a large multimodal multilingual dataset.",
    "Volume": "6,477,255",
    "Unit": "sentences",
    "Ethical Risks": "Low",
    "Provider": "Google",
    "Derived From": "",
    "Paper Title": "WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning",
    "Paper Link": "https://arxiv.org/pdf/2103.01913.pdf",
    "Script": "Arab-Latn",
    "Tokenized": "No",
    "Host": "HuggingFace",
    "Access": "Free",
    "Cost": "",
    "Test Split": "No",
    "Tasks": "image captioning, text retrieval",
    "Venue Title": "SIGIR '21",
    "Citations": "40.0",
    "Venue Type": "conference",
    "Venue Name": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "Authors": "Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, Marc Najork",
    "Affiliations": "Google",
    "Abstract": "The milestone improvements brought about by deep representation learning and pre-training techniques have led to large performance gains across downstream NLP, IR and Vision tasks. Multimodal modeling techniques aim to leverage large high-quality visio-linguistic datasets for learning complementary information (across image and text modalities). In this paper, we introduce the Wikipedia-based Image Text (WIT) Dataset (this https URL) to better facilitate multimodal, multilingual learning. WIT is composed of a curated set of 37.6 million entity rich image-text examples with 11.5 million unique images across 108 Wikipedia languages. Its size enables WIT to be used as a pretraining dataset for multimodal models, as we show when applied to downstream tasks such as image-text retrieval. WIT has four main and unique advantages. First, WIT is the largest multimodal dataset by the number of image-text examples by 3x (at the time of writing). Second, WIT is massively multilingual (first of its kind) with coverage over 100+ languages (each of which has at least 12K examples) and provides cross-lingual texts for many images. Third, WIT represents a more diverse set of concepts and real world entities relative to what previous datasets cover. Lastly, WIT provides a very challenging real-world test set, as we empirically illustrate using an image-text retrieval task as an example.",
    "Added By": "Khalid N. Elmadani"
}