{
    "Name": "wikimedia/wit_base",
    "Subsets": [],
    "HF Link": "https://hf.co/datasets/wikimedia/wit_base",
    "Link": "https://github.com/google-research-datasets/wit",
    "License": "CC BY-SA 4.0",
    "Year": 2021,
    "Language": "multilingual",
    "Dialect": "Modern Standard Arabic",
    "Domain": [
        "wikipedia"
    ],
    "Form": "text",
    "Collection Style": [
        "crawling"
    ],
    "Description": "Wikimedia's version of the Wikipedia-based Image Text (WIT) Dataset, a large multimodal multilingual dataset.",
    "Volume": 6477255.0,
    "Unit": "sentences",
    "Ethical Risks": "Low",
    "Provider": [
        "Google"
    ],
    "Derived From": [],
    "Paper Title": "WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning",
    "Paper Link": "https://arxiv.org/pdf/2103.01913.pdf",
    "Script": "Arab-Latin",
    "Tokenized": false,
    "Host": "HuggingFace",
    "Access": "Free",
    "Cost": "",
    "Test Split": false,
    "Tasks": [
        "image captioning"
    ],
    "Venue Title": "SIGIR '21",
    "Venue Type": "conference",
    "Venue Name": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "Authors": [
        "Krishna Srinivasan",
        "Karthik Raman",
        "Jiecao Chen",
        "Michael Bendersky",
        "Marc Najork"
    ],
    "Affiliations": [
        "Google"
    ],
    "Abstract": "The milestone improvements brought about by deep representation learning and pre-training techniques have led to large performance gains across downstream NLP, IR and Vision tasks. Multimodal modeling techniques aim to leverage large high-quality visio-linguistic datasets for learning complementary information (across image and text modalities). In this paper, we introduce the Wikipedia-based Image Text (WIT) Dataset (this https URL) to better facilitate multimodal, multilingual learning. WIT is composed of a curated set of 37.6 million entity rich image-text examples with 11.5 million unique images across 108 Wikipedia languages. Its size enables WIT to be used as a pretraining dataset for multimodal models, as we show when applied to downstream tasks such as image-text retrieval. WIT has four main and unique advantages. First, WIT is the largest multimodal dataset by the number of image-text examples by 3x (at the time of writing). Second, WIT is massively multilingual (first of its kind) with coverage over 100+ languages (each of which has at least 12K examples) and provides cross-lingual texts for many images. Third, WIT represents a more diverse set of concepts and real world entities relative to what previous datasets cover. Lastly, WIT provides a very challenging real-world test set, as we empirically illustrate using an image-text retrieval task as an example.",
    "Added By": "Khalid N. Elmadani"
}