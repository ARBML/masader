{
    "Name": "GEM - XLSum",
    "Subsets": [],
    "HF Link": "https://hf.co/datasets/GEM/xlsum",
    "Link": "https://github.com/csebuetnlp/xl-sum",
    "License": "CC BY-NC-SA 4.0",
    "Year": 2021,
    "Language": "multilingual",
    "Dialect": "Modern Standard Arabic",
    "Domain": [
        "news articles"
    ],
    "Form": "text",
    "Collection Style": [
        "crawling"
    ],
    "Description": "Large-Scale Multilingual Abstractive Summarization for 44 Languages\"",
    "Volume": 46897.0,
    "Unit": "sentences",
    "Ethical Risks": "Low",
    "Provider": [
        "Multiple Institutions"
    ],
    "Derived From": [],
    "Paper Title": "XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44\nLanguages",
    "Paper Link": "https://aclanthology.org/2021.findings-acl.413.pdf",
    "Script": "Arab",
    "Tokenized": false,
    "Host": "other",
    "Access": "Free",
    "Cost": "",
    "Test Split": true,
    "Tasks": [
        "summarization"
    ],
    "Venue Title": "FINDINGS",
    "Venue Type": "conference",
    "Venue Name": "Findings of the Association for Computational Linguistics",
    "Authors": [
        "Tahmid Hasan",
        "Abhik Bhattacharjee",
        "Md. Saiful Islam",
        "Kazi Samin",
        "Yuan-Fang Li",
        "Yong-Bin Kang",
        "M. Rahman",
        "Rifat Shahriyar"
    ],
    "Affiliations": [
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        ""
    ],
    "Abstract": "Contemporary works on abstractive text summarization have focused primarily on highresource languages like English, mostly due to the limited availability of datasets for low/midresource ones. In this work, we present XLSum, a comprehensive and diverse dataset comprising 1 million professionally annotated article-summary pairs from BBC, extracted using a set of carefully designed heuristics. The dataset covers 44 languages ranging from low to high-resource, for many of which no public dataset is currently available. XL-Sum is highly abstractive, concise, and of high quality, as indicated by human and intrinsic evaluation. We fine-tune mT5, a state-of-theart pretrained multilingual model, with XLSum and experiment on multilingual and lowresource summarization tasks. XL-Sum induces competitive results compared to the ones obtained using similar monolingual datasets: we show higher than 11 ROUGE-2 scores on 10 languages we benchmark on, with some of them exceeding 15, as obtained by multilingual training. Additionally, training on low-resource languages individually also provides competitive performance. To the best of our knowledge, XL-Sum is the largest abstractive summarization dataset in terms of the number of samples collected from a single source and the number of languages covered. We are releasing our dataset and models to encourage future research on multilingual abstractive summarization. The resources can be found at https://github. com/csebuetnlp/xl-sum.",
    "Added By": "Maraim Masoud"
}