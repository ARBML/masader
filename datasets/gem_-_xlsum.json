{
    "Name": "GEM - XLSum",
    "Subsets": [],
    "HF Link": "https://hf.co/datasets/GEM/xlsum",
    "Link": "https://github.com/csebuetnlp/xl-sum",
    "License": "CC BY-NC-SA 4.0",
    "Year": 2021,
    "Language": "multilingual",
    "Dialect": "ar-MSA: (Arabic (Modern Standard Arabic))",
    "Domain": "news articles",
    "Form": "text",
    "Collection Style": "crawling and annotation(other)",
    "Description": "Large-Scale Multilingual Abstractive Summarization for 44 Languages\" ",
    "Volume": "46,897",
    "Unit": "sentences",
    "Ethical Risks": "Low",
    "Provider": "Multiple Institutions",
    "Derived From": "nan",
    "Paper Title": "XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44\nLanguages",
    "Paper Link": "https://aclanthology.org/2021.findings-acl.413.pdf",
    "Script": "Arab",
    "Tokenized": "No",
    "Host": "other",
    "Access": "Free",
    "Cost": "nan",
    "Test Split": "Yes",
    "Tasks": "summarization",
    "Venue Title": "FINDINGS",
    "Citations": "0.0",
    "Venue Type": "conference",
    "Venue Name": "Findings of the Association for Computational Linguistics",
    "Authors": "Tahmid Hasan,Abhik Bhattacharjee,Md. Saiful Islam,Kazi Samin,Yuan-Fang Li,Yong-Bin Kang,M. Rahman,Rifat Shahriyar",
    "Affiliations": ",,,,,,,",
    "Abstract": "Contemporary works on abstractive text summarization have focused primarily on highresource languages like English, mostly due to the limited availability of datasets for low/midresource ones. In this work, we present XLSum, a comprehensive and diverse dataset comprising 1 million professionally annotated article-summary pairs from BBC, extracted using a set of carefully designed heuristics. The dataset covers 44 languages ranging from low to high-resource, for many of which no public dataset is currently available. XL-Sum is highly abstractive, concise, and of high quality, as indicated by human and intrinsic evaluation. We fine-tune mT5, a state-of-theart pretrained multilingual model, with XLSum and experiment on multilingual and lowresource summarization tasks. XL-Sum induces competitive results compared to the ones obtained using similar monolingual datasets: we show higher than 11 ROUGE-2 scores on 10 languages we benchmark on, with some of them exceeding 15, as obtained by multilingual training. Additionally, training on low-resource languages individually also provides competitive performance. To the best of our knowledge, XL-Sum is the largest abstractive summarization dataset in terms of the number of samples collected from a single source and the number of languages covered. We are releasing our dataset and models to encourage future research on multilingual abstractive summarization. The resources can be found at https://github. com/csebuetnlp/xl-sum.",
    "Added By": "Maraim Masoud"
}