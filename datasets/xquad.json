{
    "Name": "xquad",
    "Subsets": [],
    "HF Link": "https://hf.co/datasets/google/xquad",
    "Link": "https://github.com/deepmind/xquad",
    "License": "CC BY-SA 4.0",
    "Year": 2019,
    "Language": "multilingual",
    "Dialect": "ar-MSA: (Arabic (Modern Standard Arabic))",
    "Domain": "other",
    "Form": "text",
    "Collection Style": "human translation",
    "Description": "a benchmark dataset for evaluating cross-lingual question answering performance. The dataset consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set of SQuAD v1.1 (Rajpurkar et al., 2016) together with their professional translations into ten languages: Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi. Consequently, the dataset is entirely parallel across 11 languages.",
    "Volume": "1,190",
    "Unit": "documents",
    "Ethical Risks": "Low",
    "Provider": "DeepMind",
    "Derived From": "SQuAD",
    "Paper Title": "On the Cross-lingual Transferability of Monolingual Representations",
    "Paper Link": "https://aclanthology.org/2020.acl-main.421.pdf",
    "Script": "Arab",
    "Tokenized": "No",
    "Host": "GitHub",
    "Access": "Free",
    "Cost": "",
    "Test Split": "Yes",
    "Tasks": "question answering",
    "Venue Title": "ACL",
    "Citations": "",
    "Venue Type": "conference",
    "Venue Name": "Association of Computation Linguistics",
    "Authors": "Mikel Artetxe\u2020, Sebastian Ruder, Dani Yogatama\n",
    "Affiliations": "HiTZ Center, University of the Basque Country; DeepMind, DeepMind",
    "Abstract": "State-of-the-art unsupervised multilingual\nmodels (e.g., multilingual BERT) have been\nshown to generalize in a zero-shot crosslingual setting. This generalization ability has\nbeen attributed to the use of a shared subword\nvocabulary and joint training across multiple\nlanguages giving rise to deep multilingual\nabstractions. We evaluate this hypothesis by\ndesigning an alternative approach that transfers a monolingual model to new languages\nat the lexical level. More concretely, we first\ntrain a transformer-based masked language\nmodel on one language, and transfer it to a\nnew language by learning a new embedding\nmatrix with the same masked language\nmodeling objective\u2014freezing parameters\nof all other layers. This approach does not\nrely on a shared vocabulary or joint training.\nHowever, we show that it is competitive with\nmultilingual BERT on standard cross-lingual\nclassification benchmarks and on a new\nCross-lingual Question Answering Dataset\n(XQuAD). Our results contradict common\nbeliefs of the basis of the generalization ability\nof multilingual models and suggest that deep\nmonolingual models learn some abstractions\nthat generalize across languages. We also\nrelease XQuAD as a more comprehensive\ncross-lingual benchmark, which comprises\n240 paragraphs and 1190 question-answer\npairs from SQuAD v1.1 translated into ten\nlanguages by professional translators.",
    "Added By": "Zaid Alyafeai"
}