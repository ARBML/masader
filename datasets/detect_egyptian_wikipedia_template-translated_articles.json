{
    "Name": "Detect Egyptian Wikipedia Template-translated Articles ",
    "Subsets": [],
    "HF Link": "https://hf.co/datasets/SaiedAlshahrani/Detect-Egyptian-Wikipedia-Articles",
    "Link": "https://github.com/SaiedAlshahrani/leveraging-corpus-metadata",
    "License": "MIT License",
    "Year": 2024,
    "Language": "ar",
    "Dialect": "Egypt",
    "Domain": "wikipedia",
    "Form": "text",
    "Collection Style": "manual curation",
    "Description": "A labeled dataset of Egyptian Arabic Wikipedia articles extracted from Wikipedia dumps 2024-01-01, along with their metadata.",
    "Volume": "755,665",
    "Unit": "documents",
    "Ethical Risks": "Medium",
    "Provider": "Clarkson University",
    "Derived From": "Egyptian Arabic Wikipedia",
    "Paper Title": "Leveraging Corpus Metadata to Detect Template-based Translation: An Exploratory Case Study of the Egyptian Arabic Wikipedia Edition",
    "Paper Link": "https://arxiv.org/pdf/2404.00565.pdf",
    "Script": "Arab",
    "Tokenized": "No",
    "Host": "HuggingFace",
    "Access": "Free",
    "Cost": "",
    "Test Split": "Yes",
    "Tasks": "language modelling, topic classification, language identification, text classification",
    "Venue Title": "OSACT6",
    "Citations": "",
    "Venue Type": "workshop",
    "Venue Name": "The 6th Workshop on Open-Source Arabic Corpora and Processing Tools",
    "Authors": "Saied Alshahrani, Hesham Haroon, Ali Elfilali, Mariama Njie, Jeanna Matthews",
    "Affiliations": "Clarkson University, Sesame Labs, Cadi Ayyad University, M&T Bank",
    "Abstract": "Wikipedia articles (content pages) are commonly used corpora in Natural Language Processing (NLP) research, especially in low-resource languages other than English. Yet, a few research studies have studied the three Arabic Wikipedia editions, Arabic Wikipedia (AR), Egyptian Arabic Wikipedia (ARZ), and Moroccan Arabic Wikipedia (ARY), and documented issues in the Egyptian Arabic Wikipedia edition regarding the massive automatic creation of its articles using template-based translation from English to Arabic without human involvement, overwhelming the Egyptian Arabic Wikipedia with articles that do not only have low-quality content but also with articles that do not represent the Egyptian people, their culture, and their dialect. In this paper, we aim to mitigate the problem of template translation that occurred in the Egyptian Arabic Wikipedia by identifying these template-translated articles and their characteristics through exploratory analysis and building automatic detection systems. We first explore the content of the three Arabic Wikipedia editions in terms of density, quality, and human contributions and utilize the resulting insights to build multivariate machine learning classifiers leveraging articles' metadata to detect the template-translated articles automatically. We then publicly deploy and host the best-performing classifier, XGBoost, as an online application called EGYPTIAN WIKIPEDIA SCANNER and release the extracted, filtered, and labeled datasets to the research community to benefit from our datasets and the online, web-based detection system. ",
    "Added By": "Saied Alshahrani"
}