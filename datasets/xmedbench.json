{
    "Name": "XMedbench",
    "Subsets": [],
    "HF Link": "https://huggingface.co/datasets/FreedomIntelligence/XMedbench",
    "Link": "https://huggingface.co/datasets/FreedomIntelligence/XMedbench",
    "License": "Apache-2.0",
    "Year": 2024,
    "Language": "multilingual",
    "Dialect": "Modern Standard Arabic",
    "Domain": [
        "public datasets"
    ],
    "Form": "text",
    "Collection Style": [
        "human annotation",
        "machine annotation"
    ],
    "Description": "A multilingual medical corpus covering six most widely spoken languages including Arabic.",
    "Volume": 1090.0,
    "Unit": "sentences",
    "Ethical Risks": "Low",
    "Provider": [
        "FreedomIntelligence"
    ],
    "Derived From": [
        "MMLU"
    ],
    "Paper Title": "Apollo: A Lightweight Multilingual Medical LLM towards Democratizing Medical AI to 6B People",
    "Paper Link": "https://arxiv.org/pdf/2403.03640",
    "Script": "Arab",
    "Tokenized": false,
    "Host": "HuggingFace",
    "Access": "Free",
    "Cost": "",
    "Test Split": false,
    "Tasks": [
        "multiple choice question answering"
    ],
    "Venue Title": "arXiv",
    "Venue Type": "preprint",
    "Venue Name": "arXiv",
    "Authors": [
        "Xidong Wang",
        "Nuo Chen",
        "Junying Chen",
        "Yidong Wang",
        "Guorui Zhen",
        "Chunxian Zhang",
        "Xiangbo Wu",
        "Yan Hu",
        "Anningzhe Gao",
        "Xiang Wan",
        "Haizhou Li",
        "Benyou Wang"
    ],
    "Affiliations": [
        "The Chinese University of Hong Kong, Shenzhen",
        "Shenzhen Research Institute of Big Data"
    ],
    "Abstract": "Despite the vast repository of global medical knowledge predominantly being in English, local languages are crucial for delivering tailored healthcare services. To extend the reach of medical AI advancements to a broader population, we develop medical LLMs across the six most widely spoken languages, encompassing a global population of 6.1 billion. This effort culminates in the creation of the ApolloCorpora multilingual medical dataset and the XMedBench benchmark. The released Apollo models achieve the best performance among models of equivalent size, with Apollo-7B being the state-of-the-art multilingual medical LLM up to 70B. We will open-source training corpora, code, model weights and evaluation benchmark.",
    "Added By": "Zaid Alyafeai"
}