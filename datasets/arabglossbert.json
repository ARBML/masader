{
    "Name": "ArabGlossBERT",
    "Subsets": [],
    "HF Link": "",
    "Link": "https://huggingface.co/spaces/SinaLab/ArabGlossBERT/tree/main",
    "License": "CC BY 4.0",
    "Year": 2021,
    "Language": "ar",
    "Dialect": "Modern Standard Arabic",
    "Domain": "other",
    "Form": "text",
    "Collection Style": "manual curation",
    "Description": "It is a fine-tuned BERT model designed for Arabic Word Sense Disambiguation (WSD). The model treats WSD as a sentence-pair binary classification task, leveraging a custom dataset of 167k labeled Arabic context-gloss pairs extracted from the Arabic Ontology. Each pair is labeled as True or False, with target words identified and annotated. The dataset was used to fine-tune three pre-trained Arabic BERT models, incorporating supervised signals to emphasize target words in context. The model achieved an accuracy of 84%, demonstrating strong performance despite the use of a large set of senses.",
    "Volume": "167",
    "Unit": "sentences",
    "Ethical Risks": "Low",
    "Provider": "SinaLab, Birzeit University",
    "Derived From": "",
    "Paper Title": "ArabGlossBERT: Fine-Tuning BERT on Context-Gloss Pairs for WSD.",
    "Paper Link": "https://arxiv.org/abs/2205.09685",
    "Script": "Arab",
    "Tokenized": "No",
    "Host": "SinaLab Resources",
    "Access": "Free",
    "Cost": "",
    "Test Split": "No",
    "Tasks": "language modeling, natural language inference",
    "Venue Title": "",
    "Citations": "",
    "Venue Type": "",
    "Venue Name": "",
    "Authors": "Moustafa Al-Hajj, Mustafa Jarrar",
    "Affiliations": "",
    "Abstract": "Using pre-trained transformer models such as BERThas proven to be effective in many NLP tasks. This paper presents our work to finetune BERT models for Arabic Word Sense Disambiguation (WSD). We treated the WSD task as a sentence-pair binary classification task. First, we constructed a dataset of labeled Arabic context-gloss pairs (\u223c167k pairs) we extracted from the Arabic Ontology and the large lexicographic database available at Birzeit University. Each pair was labeled as True or False and target words in each context were identified and annotated. Second, we used this dataset for fine-tuning three pretrained Arabic BERT models. Third, we experimented the use of different supervised signals used to emphasize target words in context. Our experiments achieved promising results (accuracy of 84%) although we used a large set of senses in the experiment.",
    "Added By": "Tymaa Hammouda"
}