{
    "Name": "XTREME",
    "Subsets": [],
    "HF Link": "https://hf.co/datasets/google/xtreme",
    "Link": "https://github.com/google-research/xtreme",
    "License": "Apache-2.0",
    "Year": 2020,
    "Language": "multilingual",
    "Dialect": "Modern Standard Arabic",
    "Domain": [
        "other"
    ],
    "Form": "text",
    "Collection Style": [
        "other"
    ],
    "Description": "contains many datasets from different benchmarks like XNLI, TYDiQA, etc.",
    "Volume": 0.0,
    "Unit": "sentences",
    "Ethical Risks": "Low",
    "Provider": [
        "Google"
    ],
    "Derived From": [
        "Tydiqa",
        "xnli",
        "etc."
    ],
    "Paper Title": "XTREME: A Massively Multilingual Multi-task Benchmark\nfor Evaluating Cross-lingual Generalization",
    "Paper Link": "https://arxiv.org/pdf/2003.11080.pdf",
    "Script": "Arab",
    "Tokenized": false,
    "Host": "GitHub",
    "Access": "Free",
    "Cost": "",
    "Test Split": true,
    "Tasks": [
        "natural language inference",
        "part of speech tagging",
        "named entity recognition",
        "question answering",
        "machine translation"
    ],
    "Venue Title": "ICML",
    "Venue Type": "conference",
    "Venue Name": "International Conference on Machine Learning",
    "Authors": [
        "Junjie Hu",
        "Sebastian Ruder",
        "Aditya Siddhant",
        "Graham Neubig",
        "Orhan Firat",
        "M. Johnson"
    ],
    "Affiliations": [
        "",
        "DeepMind",
        "",
        "",
        "",
        ""
    ],
    "Abstract": "Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.",
    "Added By": "Zaid Alyafeai"
}