{
    "Name": "WikiANN NER(MMNER)",
    "Subsets": [],
    "HF Link": "https://hf.co/datasets/unimelb-nlp/wikiann",
    "Link": "https://github.com/afshinrahimi/mmner",
    "License": "unknown",
    "Year": 2019,
    "Language": "multilingual",
    "Dialect": "Modern Standard Arabic",
    "Domain": "wikipedia",
    "Form": "text",
    "Collection Style": "crawling",
    "Description": "Cross-lingual name tagging and linking for 282 languages",
    "Volume": "30,000",
    "Unit": "tokens",
    "Ethical Risks": "Low",
    "Provider": "The university of Melbourne",
    "Derived From": "",
    "Paper Title": "Massively Multilingual Transfer for NER",
    "Paper Link": "https://aclanthology.org/P19-1015.pdf",
    "Script": "Arab",
    "Tokenized": "Yes",
    "Host": "GitHub",
    "Access": "Free",
    "Cost": "",
    "Test Split": "Yes",
    "Tasks": "named entity recognition",
    "Venue Title": "ACL",
    "Citations": "54.0",
    "Venue Type": "conference",
    "Venue Name": "Assofications of computation linguisitcs",
    "Authors": "Afshin Rahimi,Yuan Li,Trevor Cohn",
    "Affiliations": "University of Melbourne,,University of Melbourne",
    "Abstract": "In cross-lingual transfer, NLP models over one or more source languages are applied to a low-resource target language. While most prior work has used a single source model or a few carefully selected models, here we consider a \u201cmassive\u201d setting with many such models. This setting raises the problem of poor transfer, particularly from distant languages. We propose two techniques for modulating the transfer, suitable for zero-shot or few-shot learning, respectively. Evaluating on named entity recognition, we show that our techniques are much more effective than strong baselines, including standard ensembling, and our unsupervised method rivals oracle selection of the single best individual model.",
    "Added By": "Maraim Masoud"
}