{
    "Name": "MultiTACRED",
    "Subsets": [],
    "HF Link": "https://hf.co/datasets/DFKI-SLT/multitacred",
    "Link": "https://hf.co/datasets/DFKI-SLT/multitacred",
    "License": "LDC User Agreement for Non-Members",
    "Year": 2021,
    "Language": "multilingual",
    "Dialect": "Modern Standard Arabic",
    "Domain": "other",
    "Form": "text",
    "Collection Style": "machine annotation",
    "Description": "MultiTACRED is a multilingual version of the large-scale TAC Relation Extraction Dataset. It covers 12 typologically diverse languages from 9 language families, and was created by the Speech & Language Technology group of DFKI by machine-translating the instances of the original TACRED dataset and automatically projecting their entity annotations. ",
    "Volume": "105,663",
    "Unit": "sentences",
    "Ethical Risks": "Low",
    "Provider": "",
    "Derived From": "TACRED",
    "Paper Title": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset",
    "Paper Link": "https://aclanthology.org/2023.acl-long.210.pdf",
    "Script": "Arab",
    "Tokenized": "No",
    "Host": "HuggingFace",
    "Access": "Free",
    "Cost": "",
    "Test Split": "Yes",
    "Tasks": "relation extraction",
    "Venue Title": "ACL",
    "Citations": "",
    "Venue Type": "conference",
    "Venue Name": "Associations of computation linguistics",
    "Authors": "",
    "Affiliations": "",
    "Abstract": "Relation extraction (RE) is a fundamental\ntask in information extraction, whose extension to multilingual settings has been hindered\nby the lack of supervised resources comparable in size to large English datasets such as\nTACRED (Zhang et al., 2017). To address this\ngap, we introduce the MultiTACRED dataset,\ncovering 12 typologically diverse languages\nfrom 9 language families, which is created by\nmachine-translating TACRED instances and\nautomatically projecting their entity annotations. We analyze translation and annotation\nprojection quality, identify error categories, and\nexperimentally evaluate fine-tuned pretrained\nmono- and multilingual language models in\ncommon transfer learning scenarios. Our analyses show that machine translation is a viable\nstrategy to transfer RE instances, with native\nspeakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE\nmodel performance to be comparable to the\nEnglish original for many of the target languages, and that multilingual models trained\non a combination of English and target language data can outperform their monolingual\ncounterparts. However, we also observe a variety of translation and annotation projection\nerrors, both due to the MT systems and linguistic features of the target languages, such\nas pronoun-dropping, compounding and inflection, that degrade",
    "Added By": "Zaid Alyafeai"
}