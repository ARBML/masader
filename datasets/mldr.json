{
    "Name": "MLDR",
    "Subsets": [],
    "HF Link": "https://hf.co/datasets/Shitao/MLDR",
    "Link": "https://hf.co/datasets/Shitao/MLDR",
    "License": "MIT License",
    "Year": 2024,
    "Language": "multilingual",
    "Dialect": "Modern Standard Arabic",
    "Domain": [
        "other"
    ],
    "Form": "text",
    "Collection Style": [
        "other"
    ],
    "Description": "MLDR is a Multilingual Long-Document Retrieval dataset built on Wikipeida, Wudao and mC4, covering 13 typologically diverse languages. Specifically, we sample lengthy articles from Wikipedia, Wudao and mC4 datasets and randomly choose paragraphs from them. Then we use GPT-3.5 to generate questions based on these paragraphs. The generated question and the sampled article constitute a new text pair to the dataset. The prompt for GPT3.5 is \u201cYou are a curious AI assistant, please generate one specific and valuable question based on the following text.",
    "Volume": 7607.0,
    "Unit": "documents",
    "Ethical Risks": "Low",
    "Provider": [],
    "Derived From": [],
    "Paper Title": "M3-Embedding: Multi-Linguality, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation",
    "Paper Link": "https://arxiv.org/pdf/2402.03216",
    "Script": "Arab",
    "Tokenized": false,
    "Host": "HuggingFace",
    "Access": "Free",
    "Cost": "",
    "Test Split": true,
    "Tasks": [
        "information retrieval"
    ],
    "Venue Title": "arXiv",
    "Venue Type": "preprint",
    "Venue Name": "",
    "Authors": [],
    "Affiliations": [],
    "Abstract": "In this paper, we introduce a new embedding\nmodel called M3-Embedding, which is distinguished for its versatility in Multi-Linguality,\nMulti-Functionality, and Multi-Granularity. It\nprovides a uniform support for the semantic retrieval of more than 100 working languages. It\ncan simultaneously accomplish the three common retrieval functionalities: dense retrieval,\nmulti-vector retrieval, and sparse retrieval. Besides, it is also capable of processing inputs\nof different granularities, spanning from short\nsentences to long documents of up to 8,192 tokens. The effective training of M3-Embedding\npresents a series of technical contributions. Notably, we propose a novel self-knowledge distillation approach, where the relevance scores\nfrom different retrieval functionalities can be\nintegrated as the teacher signal to enhance\nthe training quality. We also optimize the\nbatching strategy, which enables a large batch\nsize and high training throughput to improve\nthe discriminativeness of embeddings. M3-\nEmbedding exhibits a superior performance in\nour experiment, leading to new state-of-the-art\nresults on multilingual, cross-lingua",
    "Added By": "Zaid Alyafeai"
}