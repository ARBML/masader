{
    "Name": "ArabLegalEval NajizQA",
    "Subsets": [],
    "HF Link": "",
    "Link": "https://github.com/Thiqah/ArabLegalEval",
    "License": "unknown",
    "Year": 2024,
    "Language": "ar",
    "Dialect": "Modern Standard Arabic",
    "Domain": [
        "web pages"
    ],
    "Form": "text",
    "Collection Style": [
        "crawling",
        "human annotation"
    ],
    "Description": "filtered FQA verified by legal experts, consists of 79 question-answer pairs",
    "Volume": 79.0,
    "Unit": "sentences",
    "Ethical Risks": "Low",
    "Provider": [
        "THIQAH",
        "KAUST"
    ],
    "Derived From": [],
    "Paper Title": "ArabLegalEval: A Multitask Benchmark for Assessing Arabic Legal Knowledge in Large Language Models",
    "Paper Link": "https://arxiv.org/pdf/2408.07983",
    "Script": "Arab",
    "Tokenized": false,
    "Host": "GitHub",
    "Access": "Free",
    "Cost": "",
    "Test Split": false,
    "Tasks": [
        "question answering"
    ],
    "Venue Title": "arXiv",
    "Venue Type": "preprint",
    "Venue Name": "arXiv",
    "Authors": [
        "Faris Hijazi",
        "Somayah AlHarbi",
        "Abdulaziz AlHussein",
        "Harethah AbuShairah",
        "Reem AlZahrani",
        "Hebah AlShamlan",
        "Omar Knio",
        "George Turkiyyah"
    ],
    "Affiliations": [
        "THIQAH",
        "KAUST"
    ],
    "Abstract": "The rapid advancements in Large Language Models (LLMs) have led to significant improvements in various natural language processing tasks. However, the evaluation of LLMs' legal knowledge, particularly in non-English languages such as Arabic, remains under-explored. To address this gap, we introduce ArabLegalEval, a multitask benchmark dataset for assessing the Arabic legal knowledge of LLMs. Inspired by the MMLU and LegalBench datasets, ArabLegalEval consists of multiple tasks sourced from Saudi legal documents and synthesized questions. In this work, we aim to analyze the capabilities required to solve legal problems in Arabic and benchmark the performance of state-of-the-art LLMs. We explore the impact of in-context learning and investigate various evaluation methods. Additionally, we explore workflows for generating questions with automatic validation to enhance the dataset's quality. We benchmark multilingual and Arabic-centric LLMs, such as GPT-4 and Jais, respectively. We also share our methodology for creating the dataset and validation, which can be generalized to other domains. We hope to accelerate AI research in the Arabic Legal domain by releasing the ArabLegalEval dataset and code: https://github.com/Thiqah/ArabLegalEval",
    "Added By": "Zaid Alyafeai"
}