{
    "Name": "GEM - WikiLingua",
    "Subsets": [],
    "HF Link": "https://hf.co/datasets/esdurmus/wiki_lingua",
    "Link": "https://github.com/esdurmus/Wikilingua",
    "License": "CC0",
    "Year": 2020,
    "Language": "multilingual",
    "Dialect": "Modern Standard Arabic",
    "Domain": [
        "wikipedia"
    ],
    "Form": "text",
    "Collection Style": [
        "crawling"
    ],
    "Description": "New Benchmark Dataset for Multilingual Abstractive Summarization",
    "Volume": 29229.0,
    "Unit": "sentences",
    "Ethical Risks": "Low",
    "Provider": [
        "Multiple Institutions"
    ],
    "Derived From": [],
    "Paper Title": "WikiLingua: A New Benchmark Dataset for Cross-Lingual Abstractive Summarization",
    "Paper Link": "https://aclanthology.org/2020.findings-emnlp.360.pdf",
    "Script": "Arab",
    "Tokenized": false,
    "Host": "GitHub",
    "Access": "Free",
    "Cost": "",
    "Test Split": true,
    "Tasks": [
        "summarization"
    ],
    "Venue Title": "EMNLP",
    "Venue Type": "conference",
    "Venue Name": "Conference on Empirical Methods in Natural Language Processing",
    "Authors": [
        "Faisal Ladhak",
        "Esin Durmus",
        "Claire Cardie",
        "K. McKeown"
    ],
    "Affiliations": [
        "",
        "Stanford University",
        "",
        ""
    ],
    "Abstract": "We introduce WikiLingua, a large-scale, multilingual dataset for the evaluation of crosslingual abstractive summarization systems. We extract article and summary pairs in 18 languages from WikiHow, a high quality, collaborative resource of how-to guides on a diverse set of topics written by human authors. We create gold-standard article-summary alignments across languages by aligning the images that are used to describe each how-to step in an article. As a set of baselines for further studies, we evaluate the performance of existing cross-lingual abstractive summarization methods on our dataset. We further propose a method for direct crosslingual summarization (i.e., without requiring translation at inference time) by leveraging synthetic data and Neural Machine Translation as a pre-training step. Our method significantly outperforms the baseline approaches, while being more cost efficient during inference.",
    "Added By": "Maraim Masoud"
}